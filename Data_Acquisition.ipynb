{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39653bf8",
   "metadata": {},
   "source": [
    "# Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9b72c",
   "metadata": {},
   "source": [
    "- CHANGE the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ba98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "import re \n",
    "import os \n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from selenium import webdriver "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cef4d03",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd15a4",
   "metadata": {},
   "source": [
    "## Disclaimer: the version for the driver might need to be modifed and adjusted for the specific local machine.\n",
    "- The project can still be replicated solely in the Data_Analysis.ipynb notebook because the finalized dataset is in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826ef60",
   "metadata": {},
   "source": [
    "### Accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# web driver set up\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://info.lse.ac.uk/Staff/Departments-and-Institutes\")\n",
    "\n",
    "# Click the department\n",
    "department = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Department of Accounting')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", department)\n",
    "department.click()\n",
    "\n",
    "# Click \"People\"\n",
    "people = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'People')))\n",
    "people.click()\n",
    "people_url = driver.current_url\n",
    "\n",
    "# Click \"Academic Faculty\"\n",
    "academic_faculty = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Academic Faculty')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", academic_faculty)\n",
    "academic_faculty.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = driver.current_url\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "academic_faculty = soup.find(\"div\", attrs={'class': \"accordion__content\"})\n",
    "academic_faculty\n",
    "text_block = academic_faculty.find_all(\"div\", attrs={'class': \"accordion__txt\"})\n",
    "\n",
    "professors = []\n",
    "for professor in text_block:\n",
    "    prof_names = professor.find(\"a\", attrs={'class': \"sys_0 sys_t0\"})\n",
    "    professors.append(prof_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "professor_urls = []\n",
    "for professor in professors:\n",
    "    url = professor.get(\"href\")\n",
    "    url = \"https://www.lse.ac.uk\" + url\n",
    "    if url not in professor_urls:\n",
    "        professor_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors_dict = {}\n",
    "professor_name_list = []\n",
    "professor_prefix_list = []\n",
    "key_expertise_list = []\n",
    "professor_title_list = []\n",
    "languages_list = []\n",
    "title_list = []\n",
    "modules_list = []\n",
    "\n",
    "for one_url in professor_urls:\n",
    "    \n",
    "    r_2 = requests.get(one_url)\n",
    "    soup_2 = BeautifulSoup(r_2.content,'lxml')\n",
    "\n",
    "    # extract the prof name and prefix\n",
    "    professor = soup_2.find(\"h1\", attrs={'class': 'people__name'})\n",
    "    professor_prefix = professor.find('span', class_='people__title').text\n",
    "    professor_name = professor.text.strip(\"Dr\")\n",
    "    professor_name = professor.text.strip(\"Professor\")\n",
    "    professor_name_list.append(professor_name)\n",
    "    professor_prefix_list.append(professor_prefix)\n",
    "\n",
    "\n",
    "    # extract the key expertise and append them\n",
    "    key_expertise_locate = soup_2.find('div', class_='peopleContact__method', text=\"Key Expertise\")\n",
    "    if key_expertise_locate: # if key_expertise_locate exists and doesn't return None\n",
    "        key_expertise = key_expertise_locate.find_next_sibling('div').text\n",
    "        key_expertise_list.append(key_expertise)\n",
    "    else:\n",
    "        key_expertise_list.append(None)\n",
    "\n",
    "    # extract the languages and append them\n",
    "    languages_locate = soup_2.find('div', class_='peopleContact__method', text=\"Languages\")\n",
    "    if languages_locate:\n",
    "        languages = languages_locate.find_next_sibling('div').text\n",
    "        languages_list.append(languages)\n",
    "    else:\n",
    "        languages_list.append(None)\n",
    "\n",
    "    # Add professor title\n",
    "    professor_title = soup_2.find('h2', class_='people__position').text\n",
    "    professor_title_list.append(professor_title)\n",
    "\n",
    "    # Add courses that the professor is teaching\n",
    "    teaching = soup_2.find(name=[\"h3\",\"h2\", \"p\"], text= [\"Teaching\", \"Teaching:\"])\n",
    "    if teaching:\n",
    "        module_list = teaching.find_next('ul')\n",
    "        modules = [li.text for li in module_list.find_all('li')] # extract the items from the <li> elements\n",
    "        modules = [module.replace('\\xa0', \" \").strip(\"\\n\") for module in modules]\n",
    "        modules_list.append(modules)\n",
    "    else:\n",
    "        modules_list.append(None)\n",
    "\n",
    "professors_dict[\"Professor Name\"] = professor_name_list\n",
    "professors_dict[\"Professor Prefix\"] = professor_prefix_list\n",
    "professors_dict[\"Key Expertise\"] = key_expertise_list\n",
    "professors_dict[\"Languages\"] = languages_list\n",
    "professors_dict[\"Title\"] = professor_title_list\n",
    "professors_dict[\"Modules\"] = modules_list\n",
    "\n",
    "import pandas as pd\n",
    "professors_df = pd.DataFrame(professors_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeea34a",
   "metadata": {},
   "source": [
    "### Mathematics Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1576944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the web driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://info.lse.ac.uk/Staff/Departments-and-Institutes\")\n",
    "\n",
    "# Find and click the department\n",
    "department = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Department of Mathematics')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", department)\n",
    "department.click()\n",
    "\n",
    "# Click \"People\" section\n",
    "people = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'People')))\n",
    "people.click()\n",
    "people_url = driver.current_url\n",
    "\n",
    "# Click the \"Academic Faculty\" area\n",
    "academic_faculty = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Academic Faculty')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", academic_faculty)\n",
    "academic_faculty.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e796ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = driver.current_url\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "academic_faculty = soup.find(\"div\", attrs={'class': \"accordion__content\"})\n",
    "img_text = academic_faculty.find_all(\"div\", attrs={'class': \"accordion__imgTxt\"})\n",
    "professors = []\n",
    "#print(img_text)\n",
    "for i in img_text:\n",
    "    one_text_block = i.find(\"div\",attrs={'class': \"accordion__txt\"})\n",
    "    one_prof = one_text_block.find(\"a\",attrs={\"class\": \"sys_16\"})\n",
    "    another_prof = one_text_block.find(\"a\",attrs={\"class\": \"sys_0 sys_t0\"})\n",
    "    #if not one_prof:\n",
    "     #   one_prof = one_text_block.find(\"a\",attrs={\"class\": \"sys_0 sys_t0\"})\n",
    "    professors.append(one_prof)\n",
    "    professors.append(another_prof)\n",
    "\n",
    "professors = list(filter(None, professors))\n",
    "filtered_professors = []\n",
    "for professor in professors:\n",
    "    if professor['href'].startswith('http://www.lse.ac.uk') or professor['href'].startswith('/Mathematics'):\n",
    "        filtered_professors.append(professor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7ffd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "professor_urls = []\n",
    "for professor in filtered_professors:\n",
    "    url = professor.get(\"href\")\n",
    "    if  url.startswith('/Mathematics'):\n",
    "        url = \"http://www.lse.ac.uk\" + url\n",
    "    if url not in professor_urls:\n",
    "        professor_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9397f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors_dict = {}\n",
    "professor_name_list = []\n",
    "professor_prefix_list = []\n",
    "key_expertise_list = []\n",
    "professor_title_list = []\n",
    "languages_list = []\n",
    "title_list = []\n",
    "modules_list = []\n",
    "\n",
    "for one_url in professor_urls:\n",
    "    \n",
    "    r_2 = requests.get(one_url)\n",
    "    soup_2 = BeautifulSoup(r_2.content,'lxml')\n",
    "    # extract the prof name and prefix    \n",
    "    professor = soup_2.find(\"h1\", attrs={'class': 'people__name'})\n",
    "    professor_prefix = professor.find('span', class_='people__title').text\n",
    "    professor_name = professor.text.strip(\"Dr\")\n",
    "    professor_name = professor.text.strip(\"Professor\")\n",
    "    professor_name_list.append(professor_name)\n",
    "    professor_prefix_list.append(professor_prefix)\n",
    "\n",
    "    # extract the key expertise and append them        \n",
    "    key_expertise_locate = soup_2.find('div', class_='peopleContact__method', text=\"Key Expertise\")\n",
    "    if key_expertise_locate: # if key_expertise_locate exists and doesn't return None\n",
    "        key_expertise = key_expertise_locate.find_next_sibling('div').text\n",
    "        key_expertise_list.append(key_expertise)\n",
    "    else:\n",
    "        key_expertise_list.append(None)\n",
    "\n",
    "    # extract the languages and append them    \n",
    "    languages_locate = soup_2.find('div', class_='peopleContact__method', text=\"Languages\")\n",
    "    if languages_locate:\n",
    "        languages = languages_locate.find_next_sibling('div').text\n",
    "        languages_list.append(languages)\n",
    "    else:\n",
    "        languages_list.append(None)\n",
    "\n",
    "    # Add professor title\n",
    "    professor_title = soup_2.find('h2', class_='people__position').text\n",
    "    professor_title_list.append(professor_title)\n",
    "\n",
    "    # Add courses that the professor is teaching\n",
    "    teaching = soup_2.find(name=[\"h3\",\"h2\", \"p\"], text= [\"Teaching\", \"Teaching:\"])\n",
    "    if teaching:\n",
    "        module_list = teaching.find_next('ul')\n",
    "        modules = [li.text for li in module_list.find_all('li')] # extract the items from the <li> elements\n",
    "        modules = [module.replace('\\xa0', \" \").strip(\"\\n\") for module in modules]\n",
    "        modules_list.append(modules)\n",
    "    else:\n",
    "        modules_list.append(None)\n",
    "\n",
    "professors_dict[\"Professor Name\"] = professor_name_list\n",
    "professors_dict[\"Professor Prefix\"] = professor_prefix_list\n",
    "professors_dict[\"Key Expertise\"] = key_expertise_list\n",
    "professors_dict[\"Languages\"] = languages_list\n",
    "professors_dict[\"Title\"] = professor_title_list\n",
    "professors_dict[\"Modules\"] = modules_list\n",
    "\n",
    "professors_df = pd.DataFrame(professors_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1566fca6",
   "metadata": {},
   "source": [
    "### Finance Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf35b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://info.lse.ac.uk/Staff/Departments-and-Institutes\")\n",
    "\n",
    "# Identify and click the department\n",
    "department = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Department of Finance')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", department)\n",
    "department.click()\n",
    "\n",
    "# Click the People\n",
    "people = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'People')))\n",
    "people.click()\n",
    "people_url = driver.current_url\n",
    "\n",
    "# Find and then click \"Academic Faculty\"\n",
    "academic_faculty = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Finance faculty')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", academic_faculty)\n",
    "academic_faculty.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4cc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = driver.current_url\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "academic_faculty = soup.find(\"div\", attrs={'class': \"accordion__content\"})\n",
    "academic_faculty\n",
    "text_block = academic_faculty.find_all(\"div\", attrs={'class': \"accordion__txt\"})\n",
    "\n",
    "professors = []\n",
    "for professor in text_block:\n",
    "    prof_names = professor.find(\"a\", attrs={'class': \"sys_0 sys_t0\"})\n",
    "    professors.append(prof_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "professor_urls = []\n",
    "for professor in professors:\n",
    "    url = professor.get(\"href\")\n",
    "    url = \"https://www.lse.ac.uk\" + url\n",
    "    if url not in professor_urls:\n",
    "        professor_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1bf4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors_dict = {}\n",
    "professor_name_list = []\n",
    "professor_prefix_list = []\n",
    "key_expertise_list = []\n",
    "professor_title_list = []\n",
    "languages_list = []\n",
    "title_list = []\n",
    "modules_list = []\n",
    "\n",
    "for one_url in professor_urls:\n",
    "    \n",
    "    r_2 = requests.get(one_url)\n",
    "    soup_2 = BeautifulSoup(r_2.content,'lxml')\n",
    "\n",
    "    # extract the prof name and prefix    \n",
    "    professor = soup_2.find(\"h1\", attrs={'class': 'people__name'})\n",
    "    professor_prefix = professor.find('span', class_='people__title').text\n",
    "    professor_name = professor.text.strip(\"Dr\")\n",
    "    professor_name = professor.text.strip(\"Professor\")\n",
    "    professor_name_list.append(professor_name)\n",
    "    professor_prefix_list.append(professor_prefix)\n",
    "\n",
    "    # extract the key expertise and append them        \n",
    "    key_expertise_locate = soup_2.find('h2', text=\"Research Interests\")\n",
    "    if key_expertise_locate: # if key_expertise_locate exists and doesn't return None\n",
    "        key_expertise = key_expertise_locate.find_next_sibling('p')\n",
    "        text = key_expertise.get_text(separator='\\n')\n",
    "        key_expertise_inner_list = text.strip().split('\\n')\n",
    "        key_expertise_list.append(key_expertise_inner_list)\n",
    "    else:\n",
    "        key_expertise_list.append(None)\n",
    "\n",
    "    # extract the languages and append them        \n",
    "    languages_locate = soup_2.find('div', class_='peopleContact__method', text=\"Languages\")\n",
    "    if languages_locate:\n",
    "        languages = languages_locate.find_next_sibling('div').text\n",
    "        languages_list.append(languages)\n",
    "    else:\n",
    "        languages_list.append(None)\n",
    "\n",
    "    \n",
    "    # Adding professor title  \n",
    "    professor_title = soup_2.find('h2', class_='people__position').text\n",
    "    professor_title_list.append(professor_title)\n",
    "\n",
    "    # Add courses / modules that the professor is teaching \n",
    "    teaching = soup_2.find(name=[\"h3\",\"h2\", \"p\"], text= [\"Teaching\", \"Teaching:\"])\n",
    "    if teaching:\n",
    "        module_list = teaching.find_next('ul')\n",
    "        modules = [li.text for li in module_list.find_all('li')] # extract the items from the <li> elements\n",
    "        modules = [module.replace('\\xa0', \" \").strip(\"\\n\") for module in modules]\n",
    "        modules_list.append(modules)\n",
    "    else:\n",
    "        modules_list.append(None)\n",
    "\n",
    "professors_dict[\"Professor Name\"] = professor_name_list\n",
    "professors_dict[\"Professor Prefix\"] = professor_prefix_list\n",
    "professors_dict[\"Key Expertise\"] = key_expertise_list\n",
    "professors_dict[\"Languages\"] = languages_list\n",
    "professors_dict[\"Title\"] = professor_title_list\n",
    "professors_dict[\"Modules\"] = modules_list\n",
    "\n",
    "professors_df = pd.DataFrame(professors_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965d8a99",
   "metadata": {},
   "source": [
    "### Statistics Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the web driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://info.lse.ac.uk/Staff/Departments-and-Institutes\")\n",
    "\n",
    "# identify and click the department\n",
    "department = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Department of Statistics')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", department)\n",
    "department.click()\n",
    "\n",
    "# identify and click \"People\"\n",
    "people = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'People')))\n",
    "people.click()\n",
    "people_url = driver.current_url\n",
    "\n",
    "# identify and click \"Academic Faculty\"\n",
    "academic_faculty = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Academic faculty')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", academic_faculty)\n",
    "academic_faculty.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c67666",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = driver.current_url\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content,'lxml')\n",
    "academic_faculty = soup.find(\"div\", attrs={'class': \"accordion__content\"})\n",
    "academic_faculty\n",
    "text_block = academic_faculty.find_all(\"div\", attrs={'class': \"accordion__txt\"})\n",
    "\n",
    "professors = []\n",
    "for professor in text_block:\n",
    "    prof_names = professor.find(\"a\", attrs={'class': \"sys_0 sys_t0\"})\n",
    "    professors.append(prof_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b935d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "professor_urls = []\n",
    "for professor in professors:\n",
    "    url = professor.get(\"href\")\n",
    "    url = \"https://www.lse.ac.uk\" + url\n",
    "    if url not in professor_urls:\n",
    "        professor_urls.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "professors_dict = {} \n",
    "professor_name_list = [] \n",
    "professor_prefix_list = [] \n",
    "key_expertise_list = [] \n",
    "professor_title_list = [] \n",
    "languages_list = [] \n",
    "title_list = [] \n",
    "modules_list = [] \n",
    "\n",
    "for one_url in professor_urls: \n",
    "    \n",
    "    r_2 = requests.get(one_url) \n",
    "    soup_2 = BeautifulSoup(r_2.content,'lxml') \n",
    " \n",
    "    # Get professor prefix and name \n",
    "    professor = soup_2.find(\"h1\", attrs={'class': 'people__name'}) \n",
    "    professor_prefix = professor.find('span', class_='people__title').text \n",
    "    professor_name = professor.text.strip(\"Dr\") \n",
    "    professor_name = professor.text.strip(\"Professor\") \n",
    "    professor_name_list.append(professor_name) \n",
    "    professor_prefix_list.append(professor_prefix) \n",
    "\n",
    "    # Get the key expertise \n",
    "    key_expertise_locate = soup_2.find('div', class_='peopleContact__method', text=\"Key Expertise\") \n",
    "    if key_expertise_locate: # if key_expertise_locate exists and doesn't return None \n",
    "        key_expertise = key_expertise_locate.find_next_sibling('div').text \n",
    "        key_expertise_list.append(key_expertise) \n",
    "    else:\n",
    "        key_expertise_list.append(None) \n",
    "\n",
    "    # Get the languages \n",
    "    languages_locate = soup_2.find('div', class_='peopleContact__method', text=\"Languages\") \n",
    "    if languages_locate: \n",
    "        languages = languages_locate.find_next_sibling('div').text \n",
    "        languages_list.append(languages) \n",
    "    else: \n",
    "        languages_list.append(None) \n",
    "\n",
    "    \n",
    "    # Get professor title \n",
    "    professor_title = soup_2.find('h2', class_='people__position').text \n",
    "    professor_title_list.append(professor_title) \n",
    "\n",
    "    # Get courses that the professor is teaching \n",
    "    pattern = r\"ST\\d{3}\"\n",
    "    text = soup_2.find('div', class_='people__bio').text\n",
    "    modules = re.findall(pattern, text)\n",
    "    modules_list.append(modules)\n",
    "    \n",
    "professors_dict[\"Professor Name\"] = professor_name_list\n",
    "professors_dict[\"Professor Prefix\"] = professor_prefix_list\n",
    "professors_dict[\"Key Expertise\"] = key_expertise_list\n",
    "professors_dict[\"Languages\"] = languages_list\n",
    "professors_dict[\"Title\"] = professor_title_list\n",
    "professors_dict[\"Modules\"] = modules_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3e9677",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b176282d",
   "metadata": {},
   "source": [
    "### Economics Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097dd94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 https://www.lse.ac.uk//economics/people/faculty/keyu-jin WebpagesPersonal  |  LSE Experts  |  CFM\n",
      "1 https://www.lse.ac.uk//economics/people/faculty/jonathan-leape WebpagesPersonal  |  LSE Experts  |  IGC\n",
      "1 https://www.lse.ac.uk//economics/people/faculty/john-moore WebpagesLSE Experts  |  STICERD\n",
      "2 https://www.lse.ac.uk//economics/people/faculty/junius-olivier WebpagesPersonal\n",
      "3 https://www.lse.ac.uk//economics/people/faculty/nicolo-rosetti EducationPhD in Economics, Kyoto University\n",
      "1 https://www.lse.ac.uk//economics/people/faculty/thomas-sampson WebpagesPersonal  |  CEP  |  SPP\n",
      "1 https://www.lse.ac.uk//economics/people/faculty/judith-shapiro WebpagesPersonal\n",
      "1 https://www.lse.ac.uk//economics/people/faculty/johannes-spinnewijn WebpagesPersonal  |  CEP  |  STICERD  |  SPP\n",
      "1 https://www.lse.ac.uk//economics/people/faculty/silvana-tenreyro WebpagesPersonal  |  CFM  |  IGC\n"
     ]
    }
   ],
   "source": [
    "ec_link = 'https://www.lse.ac.uk/economics/people/faculty'\n",
    "\n",
    "response_html = requests.get(ec_link)\n",
    "main_soup = BeautifulSoup(response_html.text)\n",
    "\n",
    "\n",
    "module_codes = []\n",
    "ec_staff = []\n",
    "names = []\n",
    "titles = []\n",
    "key_exp = []\n",
    "langs = []\n",
    "mods = []\n",
    "\n",
    "for i in range(0, 70):\n",
    "    element = main_soup.find_all('a', {'class': 'sys_0 sys_t0'})[i]\n",
    "    href = element.get('href')\n",
    "    link = 'https://www.lse.ac.uk/' + href\n",
    "    ec_staff.append(link)\n",
    "\n",
    "for staff_link in ec_staff:\n",
    "    response_html = requests.get(staff_link)\n",
    "    soup = BeautifulSoup(response_html.text)\n",
    "\n",
    "    # Name\n",
    "    name = soup.find('h1').get_text()\n",
    "    names.append(name)\n",
    "\n",
    "    # Title\n",
    "    title = soup.find('h2').get_text()\n",
    "    titles.append(title)\n",
    "\n",
    "    # Key Expertise\n",
    "    key = soup.find_all('div', attrs={'class': 'peopleContact__address'})[-1].get_text()\n",
    "    key_exp.append(key)\n",
    "\n",
    "    # Languages\n",
    "    lan = soup.find_all('div', attrs={'class': 'peopleContact__address'})[-2].get_text()\n",
    "    langs.append(lan)\n",
    "\n",
    "    # Modules\n",
    "    try:\n",
    "\n",
    "        mod1 = soup.find_all('div', attrs={'class': 'people__bio'})[0].find_all('p')[0]\n",
    "        mod_txt = mod1.get_text()\n",
    "        mod_txt = mod_txt.replace('\\xa0', ' ')\n",
    "        if ('Teaching' in mod_txt) or mod_txt.startswith('Teaching'):\n",
    "\n",
    "            module_codes = re.findall(r\"(([A-Z]{2}\\d{3}): .+?(?=EC\\d{3}|$))\", mod_txt)\n",
    "            module_code = [f\"{code}\" for code, title in module_codes]\n",
    "            mods.append(module_code)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                mod1 = soup.find_all('div', attrs={'class': 'people__bio'})[0].find_all('p')[1]\n",
    "                mod_txt = mod1.get_text()\n",
    "                mod_txt = mod_txt.replace('\\xa0', ' ')\n",
    "                if ('Teaching' in mod_txt) or mod_txt.startswith('Teaching'):\n",
    "                    module_codes = re.findall(r\"(([A-Z]{2}\\d{3}): .+?(?=[A-Z]{2}\\d{3}|$))\", mod_txt)\n",
    "                    module_code = [f\"{code}\" for code, title in module_codes]\n",
    "                    mods.append(module_code)\n",
    "\n",
    "                else:\n",
    "                    try:\n",
    "                        mod1 = soup.find_all('div', attrs={'class': 'people__bio'})[0].find_all('p')[2]\n",
    "                        mod_txt = mod1.get_text()\n",
    "                        mod_txt = mod_txt.replace('\\xa0', ' ')\n",
    "                        if ('Teaching' in mod_txt) or mod_txt.startswith('Teaching'):\n",
    "                            module_codes = re.findall(r\"(([A-Z]{2}\\d{3}): .+?(?=[A-Z]{2}\\d{3}|$))\", mod_txt)\n",
    "                            module_code = [f\"{code}\" for code, title in module_codes]\n",
    "                            mods.append(module_code)\n",
    "                            \n",
    "                        else:\n",
    "                            try:\n",
    "                                mod1 = soup.find_all('div', attrs={'class': 'people__bio'})[0].find_all('p')[3]\n",
    "                                mod_txt = mod1.get_text()\n",
    "                                mod_txt = mod_txt.replace('\\xa0', ' ')\n",
    "                                if ('Teaching' in mod_txt) or mod_txt.startswith('Teaching'):\n",
    "                                    module_codes = re.findall(r\"(([A-Z]{2}\\d{3}): .+?(?=[A-Z]{2}\\d{3}|$))\", mod_txt)\n",
    "                                    module_code = [f\"{code}\" for code, title in module_codes]\n",
    "                                    mods.append(module_code)\n",
    "                                else:\n",
    "                                    mods.append('NA')\n",
    "                                    \n",
    "                            except IndexError:\n",
    "                                mods.append('NA')\n",
    "                                print(\"1\",staff_link,mod_txt)\n",
    "                                continue\n",
    "\n",
    "                    except IndexError:\n",
    "                        mods.append('NA')\n",
    "                        print(\"2\",staff_link,mod_txt)\n",
    "                        continue\n",
    "\n",
    "            except IndexError:\n",
    "                mods.append('NA')\n",
    "                print(\"3\",staff_link,mod_txt)\n",
    "                continue\n",
    "\n",
    "    except IndexError: # Thesea re to account for IndexErrors and to make sure we navigate to the right block\n",
    "        mods.append('NA')\n",
    "        print(\"4\",staff_link)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "49e672e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mods[4] = ['EC201: Microeconomic Principles I','PP450 Public Organisations: Theory and Practice']\n",
    "mods[12] = ['EC417 Advanced Macroeconomics','EC539 Macroeconomics for Research Students']\n",
    "mods[19] = ['EC402: Econometrics','EC443: Econometrics for MRes Students']\n",
    "mods[20] = ['EC1B1 Macroeconomics I','EC539 Macroeconomics for Research Students']\n",
    "mods[26] = ['EC202 Microeconomic Principles II','EC230 Economics in Public Policy']\n",
    "mods[35] = ['EC1B3 Macroeconomics l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c9ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "data_dict = {\n",
    "    'Name': names,\n",
    "    'Title': titles,\n",
    "    'Key Expertise': key_exp,\n",
    "    'Languages': langs,\n",
    "    'Modules': mods\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "econ_df = pd.concat([result, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f437b64",
   "metadata": {},
   "source": [
    "### Methodology Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e0ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_link = \"https://www.lse.ac.uk/Methodology/People\"\n",
    "response_html = requests.get(my_link)\n",
    "soup = BeautifulSoup(response_html.text)\n",
    "\n",
    "staff_links = []\n",
    "\n",
    "all_links = soup.find_all(\"div\", attrs={\"class\":\"accordion__panel\"})[0].find_all(\"a\", attrs={\"class\":\"sys_0 sys_t0\"})\n",
    "\n",
    "for person in all_links:\n",
    "    link = \"https://www.lse.ac.uk\" + person.get(\"href\")    \n",
    "    staff_links.append(link)\n",
    "\n",
    "names = []\n",
    "titles = []\n",
    "langs = []\n",
    "key_exp = []\n",
    "mods = []\n",
    "\n",
    "# iterating through all staff members' pages\n",
    "for link in staff_links:\n",
    "    \n",
    "    response_html = requests.get(link)\n",
    "    \n",
    "    soup = BeautifulSoup(response_html.text)\n",
    "    \n",
    "    # Name\n",
    "    name = soup.find_all('h1')[0].get_text()\n",
    "    names.append(name)\n",
    "    \n",
    "    # Title\n",
    "    title1 = soup.find_all('h1')[0].get_text()\n",
    "    titles.append(title1.split()[0])\n",
    "    \n",
    "    # Languages\n",
    "    lang = soup.find_all('div',attrs={'class':'peopleContact__address'})[-2].get_text()\n",
    "    langs.append(lang)\n",
    "    \n",
    "    # Key Expertise\n",
    "    exp = soup.find_all(\"div\", attrs={\"class\":\"peopleContact__address\"})[-1].get_text()\n",
    "    key_exp.append(exp)\n",
    "    \n",
    "    mods.append('NA')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "data_dict = {\n",
    "    'Name': names,\n",
    "    'Title': titles,\n",
    "    'Key Expertise': key_exp,\n",
    "    'Languages': langs,\n",
    "    'Modules': mods\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "my_df = pd.concat([result, df], ignore_index=True)\n",
    "my_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d25314",
   "metadata": {},
   "source": [
    "### Management Department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_link = \"https://www.lse.ac.uk/management/people-home\"\n",
    "response_html = requests.get(my_link)\n",
    "\n",
    "soup = BeautifulSoup(response_html.text)\n",
    "\n",
    "staff_links = []\n",
    "\n",
    "for i in range(0,6):\n",
    "\n",
    "    all_links = soup.find_all(\"div\", attrs={\"class\":\"accordion__panel\"})[i].find_all(\"a\", attrs={\"class\":\"sys_0 sys_t0\"})\n",
    "\n",
    "    for person in all_links:\n",
    "        link = \"https://www.lse.ac.uk\" + person.get(\"href\")    \n",
    "        staff_links.append(link)\n",
    "\n",
    "names = []\n",
    "titles = []\n",
    "langs = []\n",
    "key_exp = []\n",
    "mods = []        \n",
    "        \n",
    "for person in staff_links:        \n",
    "    response_html = requests.get(person)\n",
    "    \n",
    "    soup = BeautifulSoup(response_html.text)\n",
    "    \n",
    "    # Name\n",
    "    name = soup.find_all('h1')[0].get_text()\n",
    "    names.append(name)\n",
    "    \n",
    "    # Title\n",
    "    title1 = soup.find_all('h1')[0].get_text()\n",
    "    titles.append(title1.split()[0])\n",
    "    \n",
    "    # Languages\n",
    "    lang = soup.find_all('div',attrs={'class':'peopleContact__address'})[-2].get_text()\n",
    "    langs.append(lang)\n",
    "    \n",
    "    # Key Expertise\n",
    "    exp = soup.find_all(\"div\", attrs={\"class\":\"peopleContact__address\"})[-1].get_text()\n",
    "    key_exp.append(exp)\n",
    "    \n",
    "    mods.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "data_dict = {\n",
    "    'Name': names,\n",
    "    'Title': titles,\n",
    "    'Key Expertise': key_exp,\n",
    "    'Languages': langs,\n",
    "    'Modules': mods\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "mg_df = pd.concat([result, df], ignore_index=True)\n",
    "mg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd51661f",
   "metadata": {},
   "source": [
    "### Data Science Institute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e83ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_link = \"https://www.lse.ac.uk/DSI/People\"\n",
    "response_html = requests.get(my_link)\n",
    "\n",
    "soup = BeautifulSoup(response_html.text)\n",
    "\n",
    "staff_links = []\n",
    "for person in soup.find_all(\"div\", attrs={\"class\":\"accordion__content\"})[0].find_all(\"a\", attrs={\"class\":\"sys_0 sys_t0\"}):\n",
    "    link = \"https://www.lse.ac.uk\" + person.get(\"href\")    \n",
    "    staff_links.append(link)\n",
    "    \n",
    "names = []\n",
    "titles = []\n",
    "langs = []\n",
    "key_exp = []\n",
    "mods = []\n",
    "\n",
    "for person in staff_links:        \n",
    "    response_html = requests.get(person)\n",
    "    \n",
    "    soup = BeautifulSoup(response_html.text)\n",
    "    \n",
    "    # Name\n",
    "    name = soup.find_all('h1')[0].get_text()\n",
    "    names.append(name)\n",
    "    \n",
    "    # Title\n",
    "    title1 = soup.find_all('h1')[0].get_text()\n",
    "    titles.append(title1.split()[0])\n",
    "    \n",
    "    # Languages\n",
    "    lang = soup.find_all('div',attrs={'class':'peopleContact__address'})[-2].get_text()\n",
    "    langs.append(lang)\n",
    "    \n",
    "    # Key Expertise\n",
    "    exp = soup.find_all(\"div\", attrs={\"class\":\"peopleContact__address\"})[-1].get_text()\n",
    "    key_exp.append(exp)\n",
    "    \n",
    "    mods.append('NA')\n",
    "\n",
    "# adding the courses manually based on the website\n",
    "mods[2] = ['DS101','DS101','DS202']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe73d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "data_dict = {\n",
    "    'Name': names,\n",
    "    'Title': titles,\n",
    "    'Key Expertise': key_exp,\n",
    "    'Languages': langs,\n",
    "    'Modules': mods\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "dsi_df = pd.concat([result, df], ignore_index=True)\n",
    "dsi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c037a89",
   "metadata": {},
   "source": [
    "## Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('../data','accounting.csv'')\n",
    "professors_df.to_csv(file_path)\n",
    "\n",
    "file_path = os.path.join('../data','maths.csv'')\n",
    "professors_df.to_csv(file_path)\n",
    "\n",
    "file_path = os.path.join('../data','finance.csv'')\n",
    "professors_df.to_csv(file_path)\n",
    "\n",
    "file_path = os.path.join('../data','stats.csv'')\n",
    "professors_df.to_csv(file_path)\n",
    "                         \n",
    "file_path = os.path.join('../data','economics.csv')\n",
    "econ_df.to_csv(file_path)  \n",
    "\n",
    "file_path = os.path.join('../data', 'methodology.csv')\n",
    "my_df.to_csv(file_path)      \n",
    "\n",
    "file_path = os.path.join('../data', 'management.csv')\n",
    "mg_df.to_csv(file_path) \n",
    "                         \n",
    "file_path = os.path.join('../data', 'datascienceinstitute.csv')\n",
    "dsi_df.to_csv(file_path)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9dee92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db475e86",
   "metadata": {},
   "source": [
    "Talya - Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d402435",
   "metadata": {},
   "outputs": [],
   "source": [
    "- For the last four departments short-cut method is followed using only **Beautiful Soup**:\n",
    "\n",
    "From the webpage https://www.lse.ac.uk/{department_name}/People, the div of the \"Academic Faculty\" section is found and again the urls for each professors' website is put in a list.\n",
    "\n",
    "- The following steps for all departments uses the library **Beautiful Soup**:\n",
    "\n",
    "Finally all the links in that list are clicked one-by-one in a loop and the following variables are scraped:\n",
    "\n",
    "- professor names\n",
    "- professor titles / prefixes\n",
    "- languages they speak\n",
    "- modules they are teaching\n",
    "- key expertise\n",
    "\n",
    "The data scraped are first put in Python dictionaries and then converted to Pandas DataFrames. Finally the dataframe is turned to seperate **csv files** and stored in the \"data\" file in the \"ST115_Project\" folder. The codes for these processes can be found in the **\"Data Acquisition\"** notebook.\n",
    "\n",
    "example for accounting department:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c91a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Set up the web driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://info.lse.ac.uk/Staff/Departments-and-Institutes\")\n",
    "\n",
    "# Find and click the department\n",
    "department = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Department of Accounting')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", department)\n",
    "department.click()\n",
    "\n",
    "# Find and click \"People\"\n",
    "people = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'People')))\n",
    "people.click()\n",
    "people_url = driver.current_url\n",
    "\n",
    "# Find and click \"Academic Faculty\"\n",
    "academic_faculty = WebDriverWait(driver, 15).until(EC.element_to_be_clickable((By.LINK_TEXT, 'Academic Faculty')))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView();\", academic_faculty)\n",
    "academic_faculty.click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
